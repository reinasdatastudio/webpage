---
title: "Data Science Project: Predicting Prices of Diamond using dataset from R library"
author: "Reina Kaino"
format:
  html:
    page-layout: article
    toc: true
    toc-location: right
    include-after-body: footer-hypertension.html
---

```{=html}
<style>

section {
  align-items: start !important;
}

h2 {
    margin-top: 1rem !important;
}

#title-block-header {
  margin-bottom: 2rem;
}

</style>
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(ggplot2)
library(dplyr)
library(randomForest)
library(ranger)  # faster Random Forest implementation
library(Metrics) # for RMSE and R-squared
library(corrplot)
```

> # Introduction

This dataset contains the prices and other attributes of almost 54,000 diamonds. The dataset is used to predict the price of a diamond based on its characteristics such as carat, cut, color, clarity, and dimensions. The project involves data cleaning, exploratory data analysis, and building predictive models to understand the factors that influence diamond prices. The goal is to create a model that can accurately predict the price of a diamond based on its attributes.

## Summary of Diamonds dataset

```{r}
data(diamonds)
head(diamonds)
```

-   **Price:** Price in US dollars (Range: \$326-\$18823)
-   **Carat:** Weight of the diamond (0.2-5.01)
-   **Cut:** Quality of the cut (Fair, Good, Very Good, Premium, Ideal)
-   **Color:** Diamond colour, from D (best) to J (worst)
-   **Clarity:** A measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))
-   **x:** Length in mm (0-10.74)
-   **y:** Width in mm (0-58.9)
-   **z:** Depth in mm(0-31.8)
-   **Depth:** Total depth percentage = z/mean(x,y)=2\*z/(x+y)(43-79)
-   **Table:** Width of top of diamond relative to widest point (43-95)

> # Data Cleaning & Feature Engineering

Check for any impossible measurements (x, y, or z = 0) and remove them:

```{r}
sum(diamonds$x == 0)
sum(diamonds$y == 0)
sum(diamonds$z == 0)

diamonds <- subset(diamonds, x > 0 & y > 0 & z > 0)
```

Check for any missing values that are significant:

```{r}
sum(is.na(diamonds))
any(diamonds == "")
```

-   Create **price per carat**.
-   Apply log transformation to reduce skewness (As seen in the later observations)

```{r}

diamonds <- diamonds %>%
  mutate(price_per_carat = price / carat,
         log_price = log(price),
         log_carat = log(carat))
```

Outliers are kept for this instance as most of the outliers are assumed to be true observations - They are not data errors, they are the expensive range. Removing these from the dataset would cause bias and underestimate prices for higher-end diamonds.

> # Exploratory Data Analysis

## Price Distribution

```{r}

ggplot(diamonds, aes(price)) +
  geom_histogram(bins = 50, fill = "steelblue", color = "white") +
  labs(title = "Distribution of Diamond Prices")

hist(diamonds$carat, main = "Carat", xlab = "", col = "purple")

hist(diamonds$depth, xlim=c(43,79), main = "Depth", xlab = "", col = "purple")

hist(diamonds$table, main = "Table", xlab = "", col = "purple")

hist(diamonds$price, main = "Price", xlab = "", col = "purple")

```

```{r}
ggplot(diamonds, aes(x = cut)) +
  geom_bar(fill = "pink") +
  labs(title = "Number of Diamonds by Cut", x = "Cut", y = "Count") +
  theme_minimal()

ggplot(diamonds, aes(x = color)) +
  geom_bar(fill = "pink") +
  labs(title = "Number of Diamonds by Colour", x = "Colour", y = "Count") +
  theme_minimal()

ggplot(diamonds, aes(x = clarity)) +
  geom_bar(fill = "pink") +
  labs(title = "Number of Diamonds by Clarity", x = "Clarity", y = "Count") +
  theme_minimal()
```

## Price per Carat by Cut

```{r}
ggplot(diamonds, aes(cut, price_per_carat, fill = cut)) +
  geom_boxplot() +
  labs(title = "Price per Carat by Cut")
```

This box plot shows that the quality of cut does not have such a significant effect in price change.

## Correlation Table

```{r}
num_vars <- diamonds %>% select(price, carat, x, y, z)
cor_table <- cor(num_vars)
corrplot(cor_table, method = 'number')
```

Only numerical variables are used for correlation matrix. It makes sense that carat is highly correlated with x, y and z: the bigger the diamond the heavier. The most interesting observation here is the high correlation between price and carat.

> # Modelling

Both Linear Regression and Random Forest are used to compare the accuracy of the predictions.

log_price and log_carat were used in the linear regression model whereas Random Forest used variables without conversion. This is due to Linear Regression assumeing linearity and constant variance.

Random Forest, on the other hand, handles skewed distribution and heteroscedasticity much better. It is easier to interpret raw data.

Both of these exclude variables such as x, y, z etc. as they are variables strongly related to each other. Including them in these models may lead to multicollinearity and the over-fitting of the data.

## Train/Test Split

```{r}
set.seed(123)
n <- nrow(diamonds)
train_idx <- sample(1:n, size = 0.7*n)
train <- diamonds[train_idx, ]
test  <- diamonds[-train_idx, ]
```

## Linear Regression

```{r}
train$cut    <- factor(train$cut, ordered = FALSE)
train$color  <- factor(train$color, ordered = FALSE)
train$clarity <- factor(train$clarity, ordered = FALSE)

fit <- lm(log_price ~ log_carat + cut + color + clarity, data = train)
summary(fit)
```

The R-squared and the adjusted R-squared values are both 0.9826. 98% of the variation in log(price) is explained by the model: The regression fits the data very well.

The p-value is very small (\<0.05) meaning we can reject the null hypothesis that there are no correlations between the dependent (Price) and the independent variables.

Looking at the coefficient, along with the lowest standard error and the highest t-value, log(carat) is concluded to be the highest predictor of price. One unit of increase in carat increases price by exp(1.88) = 6.55x.

## Random Forest

`Ranger` is used for faster computation.

```{r}
set.seed(123)
train_sample <- train %>% sample_n(10000)
rf_model <- ranger(price ~ carat + cut + color + clarity,
                   data = train_sample,
                   num.trees = 100,
                   importance = 'impurity')
rf_model
```

R-squared value was 0.977. As later seen in the evaluation, the linear regression model has heteroscedasticity. This Random Forest model predicts prices more accurately especially for more expensive, bigger carat diamonds.

> # Model Evaluation

## Linear Regression Diagnostics

### Observed vs Predicted

```{r}
pred <- exp(predict(fit, newdata = test))
obs <- test$price

plot(obs, pred,
     xlab = "Observed Price", ylab = "Predicted Price",
     main = "Observed vs Predicted (Linear Regression)", pch = 19, col = rgb(0,0,1,0.4))
abline(0, 1, col = "red", lwd = 2)
```

### Residuals vs Predicted

```{r}
residuals <- obs - pred
plot(pred, residuals,
     xlab = "Predicted Price", ylab = "Residuals",
     main = "Residuals vs Predicted", pch = 19, col = rgb(1,0.5,0,0.5))
abline(h = 0, col = "red", lwd = 2)
```

### QQ-Plot of Residuals

```{r}
qqnorm(residuals, main = "QQ-Plot of Residuals")
qqline(residuals, col = "red", lwd = 2)
```

### Metrics (Linear Regression)

```{r}
rmse_lm <- rmse(obs, pred)
r2_lm <- 1 - sum((obs - pred)^2) / sum((obs - mean(obs))^2)
rmse_lm; r2_lm
```

log() was transformed back to the raw data with exp().

Although the lines shown on these graphs are straight, the dots spread out as higher the price becomes. The higher the price becomes, more errors in prediction becomes obvious. The increase in values of residuals is visible, therefore the conclusions are that:

-   Linear regression assumption of constant variance are not met

-   Model predictions are less reliable for more expensive diamonds as errors are larger

Despite these results, the residuals are small for typical diamonds, so the linear regression model predicts well in the common price range.

Random Forest handles this better because it does not assume constant variance.

## Random Forest Evaluation

```{r}
rf_pred <- predict(rf_model, data = test)$predictions
rmse_rf <- rmse(obs, rf_pred)
r2_rf <- 1 - sum((obs - rf_pred)^2) / sum((obs - mean(obs))^2)
rmse_rf; r2_rf

importance(rf_model)
```

Random Forest showed r-squared value of 0.977 which is higher than that of linear regression (0.961). RMSE (Root means squared error) was also lower for Random Forest, meaning that this model predicts the price better.

> # Conclusions

-   **Carat** is the strongest predictor of diamond price.
-   **Cut, color, and clarity** refine predictions.
-   In this case where the null hypothesis of constant variance was rejected, Random Forest performed better in accurate predictions of prices.
